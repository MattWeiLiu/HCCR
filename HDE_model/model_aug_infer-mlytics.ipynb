{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from configs.configuration import Config\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import cv2\n",
    "\n",
    "from utils.get_data import get_npy\n",
    "from utils.top_k_accuracy import accuracy\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "from glob import glob\n",
    "import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from model.resnet18_lambda1 import ResNet18_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------- APP CONFIG ----------------------------------\n",
      "model: \n",
      "  name: resnet18-lambda-mcpl-ranlars-aug-embed2_4808\n",
      "  pretrain: False\n",
      "zero_shot: False\n",
      "embed: \n",
      "  path: data/embed/Embedding_tc_0.3_0.001_0.6.csv\n",
      "loss: \n",
      "  name: MCPL\n",
      "train: \n",
      "  source: data/cha_table/tc_cha_4808.txt\n",
      "  optim: ralars\n",
      "  lr: 0.005\n",
      "  max_epoch: 100\n",
      "  batch_size: 512\n",
      "  data_shuffle: True\n",
      "test: \n",
      "  source: cha_table/cha_test-Copy1.txt\n",
      "  batch_size: 512\n",
      "  data_shuffle: False\n",
      "aug: \n",
      "  aug_func: False\n",
      "  train_amplify: 5\n",
      "  test_amplify: 1\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "config = Config(os.path.join(os.getcwd(), \"configs/resnet18-mcpl-ranlars.yaml\"))\n",
    "Embedding = pd.read_csv(config.embed.path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(imgs):\n",
    "    \n",
    "    imgs = imgs/255\n",
    "    imgs = torch.tensor(imgs, dtype=torch.float)\n",
    "    imgs = Variable(imgs.cuda())\n",
    "    imgs = torch.unsqueeze(imgs, 1)\n",
    "    \n",
    "    pred_y = model(imgs)\n",
    "    return pred_y.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet18_lambda:\n\tMissing key(s) in state_dict: \"lalayer.rel_pos_emb\". \n\tUnexpected key(s) in state_dict: \"lalayer.pos_emb\". \n\tsize mismatch for lalayer.to_q.weight: copying a param with shape torch.Size([64, 1000, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n\tsize mismatch for lalayer.to_k.weight: copying a param with shape torch.Size([64, 1000, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n\tsize mismatch for lalayer.to_v.weight: copying a param with shape torch.Size([1000, 1000, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n\tsize mismatch for lalayer.norm_v.weight: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for lalayer.norm_v.bias: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for lalayer.norm_v.running_mean: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for lalayer.norm_v.running_var: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([256]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c702152b3fb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m### reload weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'save/model_3755cha_allembed_resnet18+lambda_MCPL_ralars_aug_4808_embed2.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 830\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    831\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet18_lambda:\n\tMissing key(s) in state_dict: \"lalayer.rel_pos_emb\". \n\tUnexpected key(s) in state_dict: \"lalayer.pos_emb\". \n\tsize mismatch for lalayer.to_q.weight: copying a param with shape torch.Size([64, 1000, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n\tsize mismatch for lalayer.to_k.weight: copying a param with shape torch.Size([64, 1000, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n\tsize mismatch for lalayer.to_v.weight: copying a param with shape torch.Size([1000, 1000, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 1, 1]).\n\tsize mismatch for lalayer.norm_v.weight: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for lalayer.norm_v.bias: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for lalayer.norm_v.running_mean: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for lalayer.norm_v.running_var: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([256])."
     ]
    }
   ],
   "source": [
    "model = ResNet18_lambda(out_dim=301).cuda()\n",
    "\n",
    "### reload weights\n",
    "checkpoint = torch.load('save/model_3755cha_allembed_resnet18+lambda_MCPL_ralars_aug_4808_embed2.h5')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('data/crop3/tsmc-sample-invoices_Invoice_JPEG_Gray_HW_TC_Invoice_ (5).jpg', cv2.IMREAD_GRAYSCALE)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crops_path = glob('data/crop3/*.png')\n",
    "\n",
    "imgs = []\n",
    "raw_imgs = []\n",
    "kernel = np.ones((2,2),np.uint8)\n",
    "\n",
    "for path in crops_path:\n",
    "    crop = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    raw_imgs.append(crop)\n",
    "    \n",
    "    # erosion & dilation\n",
    "    # erosion = cv2.erode(crop, kernel, iterations = 1)\n",
    "    # dilation = cv2.dilate(erosion, kernel, iterations = 1)\n",
    "    # erosion = cv2.medianBlur(crop, 3)\n",
    "\n",
    "    resImg = cv2.resize(crop, (64,64), interpolation=cv2.INTER_CUBIC)\n",
    "    imgs.append(resImg)\n",
    "    \n",
    "\n",
    "\n",
    "pred_y = infer(np.array(imgs))\n",
    "\n",
    "pred_y = np.vstack(pred_y)\n",
    "score = cosine_similarity(pred_y, Embedding.values.T)\n",
    "top_score = score.argsort()[:,::-1][:,:10]\n",
    "\n",
    "for idx, n in enumerate(pred_y):\n",
    "    print(Embedding.columns[top_score][idx])\n",
    "#     plt.imshow(raw_imgs[idx], cmap='gray')\n",
    "#     plt.show()\n",
    "    \n",
    "    plt.imshow(imgs[idx], cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crops_path = glob('data/crop2/*.png')\n",
    "\n",
    "imgs = []\n",
    "raw_imgs = []\n",
    "kernel = np.ones((2,2),np.uint8)\n",
    "\n",
    "for path in crops_path:\n",
    "    crop = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    raw_imgs.append(crop)\n",
    "    \n",
    "    # erosion & dilation\n",
    "    # erosion = cv2.erode(crop, kernel, iterations = 1)\n",
    "    # dilation = cv2.dilate(crop, kernel, iterations = 1)\n",
    "    # erosion = cv2.medianBlur(crop, 3)\n",
    "\n",
    "    resImg = cv2.resize(crop, (64,64), interpolation=cv2.INTER_CUBIC)\n",
    "    imgs.append(resImg)\n",
    "    \n",
    "\n",
    "\n",
    "pred_y = infer(np.array(imgs))\n",
    "\n",
    "pred_y = np.vstack(pred_y)\n",
    "score = cosine_similarity(pred_y, Embedding.values.T)\n",
    "top_score = score.argsort()[:,::-1][:,:10]\n",
    "\n",
    "for idx, n in enumerate(pred_y):\n",
    "    print(Embedding.columns[top_score][idx])\n",
    "#     plt.imshow(raw_imgs[idx], cmap='gray')\n",
    "#     plt.show()\n",
    "    \n",
    "    plt.imshow(imgs[idx], cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crops_path = glob('data/crop/*.png')\n",
    "\n",
    "imgs = []\n",
    "raw_imgs = []\n",
    "kernel = np.ones((2,2),np.uint8)\n",
    "\n",
    "for path in crops_path:\n",
    "    crop = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    raw_imgs.append(crop)\n",
    "    \n",
    "    # erosion & dilation\n",
    "    # erosion = cv2.erode(crop, kernel, iterations = 1)\n",
    "    # dilation = cv2.dilate(crop, kernel, iterations = 1)\n",
    "    # erosion = cv2.medianBlur(crop, 3)\n",
    "\n",
    "    resImg = cv2.resize(crop, (64,64), interpolation=cv2.INTER_CUBIC)\n",
    "    imgs.append(resImg)\n",
    "    \n",
    "\n",
    "\n",
    "pred_y = infer(np.array(imgs))\n",
    "\n",
    "pred_y = np.vstack(pred_y)\n",
    "score = cosine_similarity(pred_y, Embedding.values.T)\n",
    "top_score = score.argsort()[:,::-1][:,:10]\n",
    "\n",
    "for idx, n in enumerate(pred_y):\n",
    "    print(Embedding.columns[top_score][idx])\n",
    "#     plt.imshow(raw_imgs[idx], cmap='gray')\n",
    "#     plt.show()\n",
    "    \n",
    "    plt.imshow(imgs[idx], cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m58",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m58"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
